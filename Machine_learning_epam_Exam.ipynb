{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrandNewMyUserName/Game-Knight-s-labirint/blob/main/Machine_learning_epam_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the rank of trained Neural Networks"
      ],
      "metadata": {
        "id": "kEWqgkD692dZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Final assignment, you're going to explore trained neural networks, and study the rank of its matrices.\n",
        "\n",
        "**Reminder**: The rank is the number of independent columns of the matrix. If a matrix $A \\in \\mathbb{R}^{n\\times m}$  has rank $k$, then $A$ can be approximated by\n",
        "\n",
        "$$A \\approx B \\cdot C$$\n",
        "\n",
        "where $B \\in \\mathbb{R}^{n\\times k}$ and $C \\in \\mathbb{R}^{k\\times m}$.\n",
        "\n",
        "You can find the rank of matrix $A$ by performing Gaussian elimination and counting the number of pivots. This can be done in few lines of `numpy` code.\n",
        "\n",
        "**References**:\n",
        "- https://arxiv.org/pdf/1804.08838\n",
        "- https://arxiv.org/pdf/2209.13569\n",
        "- https://arxiv.org/pdf/2012.13255\n",
        "\n",
        "Note: The references above are not needed to complete this notebook, but reading them might give you additional insights."
      ],
      "metadata": {
        "id": "K3VP30ED-B_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important\n",
        "\n",
        "1. For all the training done, make sure to plot things like the loss values and accuracy on each epoch.\n",
        "\n",
        "    - You can either use tensorboard or just make a static matplotlib plot.\n",
        "    \n",
        "2. Don't add biases to the layers in the network, not important for this notebook.\n",
        "3. No need to use Dropout or BatchNorm on the network.\n",
        "4. Remember to use GPUs during the training.\n",
        "5. Always test your hypothesis on both training and testing sets, you might get a surprising result sometimes."
      ],
      "metadata": {
        "id": "jUKDdnCbMLSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Downloading MNIST and Dataloaders\n",
        "\n",
        "Download the MNIST dataset and split into training and testing, and create dataloaders.\n",
        "\n",
        "Link: https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html"
      ],
      "metadata": {
        "id": "QlFG5uMx_uPR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jU4hXFXr_xJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Train a neural network\n",
        "\n",
        "Build a simple Multi-layered Perceptron with ReLU activations, and train it on MNIST until achieving 95% accuracy or higher.\n"
      ],
      "metadata": {
        "id": "vLAtp2bM-W5z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8ikfbxo914-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Analyze the rank of the matrices in this network\n",
        "\n",
        "Perform experiments and answer the following questions:\n",
        "- What's the average rank of the matrices on all layers?\n",
        "- How does the rank increase as we go to deeper layers?\n",
        "- Try the same MLP, but change the activation function to others ($\\tanh, \\sigma, \\dots$). Do the answers change?"
      ],
      "metadata": {
        "id": "7q1K0drgAIPX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtL3NVMOL6yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Overfit by scaling the MLP\n",
        "\n",
        "1. Create a bigger network and train it on MNIST, to the point of overfitting.\n",
        "2. Now check the rank of the matrices in the network, and answer the same questions."
      ],
      "metadata": {
        "id": "Htp37DfHCQKL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U1ZJwX5SCOx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Approximate low-rank\n",
        "\n",
        "From some of the references given at the beginning, you can realize that trained neural networks have intrinsically low dimensionality (meaning low-rank matrices).\n",
        "\n",
        "In this task, take the overparametrized network already trained from the TASK4 and try to approximate each layer's matrix with a product of two other low-rank matrices?\n",
        "\n",
        "This means, if a layer has a matrix $A \\in\\mathbb{R}^{n\\times m}$, then try to find two matrices $B \\in \\mathbb{R}^{n\\times r}$ and $C \\in \\mathbb{R}^{r\\times m}$ so that $\\lvert {A - B\\cdot C}\\rvert $ is minimized, where $\\lvert x\\rvert$ means the Frobenius norm. You can use a different norm, if you think it makes sense. In order to learn $B$ and $C$, you can do gradient descent-like algorithms, where you alternate between updating $B$ and $C$ on each optimization step.\n",
        "\n",
        "**Ablate**:\n",
        "Try different values for $r$ and analyze how good your approximation is (for e.g, by taking average Frobenius norm across all layers) as you increase $r$. Make a plot with that.\n",
        "\n",
        "Conclude what is the effective rank $r$: the smallest rank such that the approximation of that rank is good enough (meaning the Frobenius norm is smaller than some threshold chosen by you)."
      ],
      "metadata": {
        "id": "-jSb8aKmMDcB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUgH6MOcSIV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Learning with low-rank factorization\n",
        "\n",
        "Once you found the effective rank $r$, take the same architecture from the previous task, and now replace each layer $A \\in \\mathbb{R}^{n\\times m}$ by a layer that applies $B\\cdot C$ with $B\\in \\mathbb{R}^{n\\times r}$ and $C \\in \\mathbb{R}^{r\\times m}$.\n",
        "\n",
        "**Question**: How much memory do you save? (you can just count the number of parameters of the original network and compare to that of the new network).\n",
        "\n",
        "Initialize these values with standard initialization, and train this network.\n",
        "\n",
        "**Question**: How does the learning change? Does it converge faster or slower? What about accuracy on both training and testing sets?\n",
        "\n",
        "**Question**: Now try doing inference, how much improvement do you see?"
      ],
      "metadata": {
        "id": "PrBwf5bwPOny"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_oFw7TlRHFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Final conclusions\n",
        "\n",
        "Based on all the previous experiments, report your conclusions and try to give an explanation to the behaviours you observed.\n",
        "\n",
        "Can you think of other ways of using the low-rank factorizations? What about SVD? Provide an explanation."
      ],
      "metadata": {
        "id": "tpYV1ebrkma1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0sK2e9dlMY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BONUS Task: LoRA\n",
        "\n",
        "Propose ideas by which low-rank could improve fine-tuning and training? Which disadvantages does it have?\n",
        "\n",
        "Read about LoRA (given in one of the references at the begining of the notebook).\n",
        "\n",
        "Now, take MNIST, and remove some digit from the dataset (keep the same labels, just remove the datapoints of a specific label).\n",
        "\n",
        "Train a simple MLP on this modified dataset.\n",
        "Fine-tune in the datapoints of the chosen digit, by using LoRA.\n",
        "\n",
        "Report the memory and time overheads."
      ],
      "metadata": {
        "id": "JgH6pfFNQ196"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSJgfNlOMGxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}